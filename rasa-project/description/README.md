# Процесс обучения модели

## 1. Пользователь загружает данные (nlu.yml)

Что происходит: Пользователь предоставляет файл nlu.yml, который содержит примеры фраз (training data) для каждого намерения (intent) и, возможно, сущности (entities).

Пример данных:

```yaml
nlu:
  - intent: greet
    examples: |
      - Привет
      - Здравствуйте
      - Добрый день
```

## 2. WhitespaceTokenizer (Токенизатор)

Что происходит: Текст разбивается на токены (слова) с помощью пробелов. Это первый этап обработки текста.

Пример:

```
Вход: "Добрый день"

Выход: ["Добрый", "день"]
```

## 3. RegexFeaturizer (Извлечение признаков с помощью регулярных выражений)

Что происходит: На этом этапе из текста извлекаются признаки с использованием регулярных выражений. Например, можно выделить email, числа, даты и другие шаблоны.

Пример:

```
Вход: "Мой email: example@example.com"

Выход: Признак, указывающий на наличие email в тексте.
```

## 4. LexicalSyntacticFeaturizer (Лексико-синтаксический анализатор)

Что происходит: Из текста извлекаются лексические и синтаксические признаки, такие как часть речи, наличие заглавных букв, длина слова и т.д.

Пример:

```
Вход: "Добрый день"

Выход: Признаки, такие как [длина=6, часть_речи=прилагательное] для слова "Добрый".
```


## 5. CountVectorsFeaturizer (Извлечение признаков по частоте слов)

Что происходит: Текст преобразуется в числовые векторы на основе частоты слов (Bag of Words). Используются N-граммы (например, биграммы), чтобы учитывать последовательности слов.

Пример:

```
Вход: ["Добрый", "день"]

Выход: Вектор, например, [1, 0, 1, 0, ...], где каждая позиция соответствует определенному слову или N-грамме.
```

## 6. LanguageModelFeaturizer (BERT)

Что происходит: Текст обрабатывается с использованием предобученной языковой модели BERT. BERT преобразует текст в контекстно-зависимые векторные представления, которые учитывают семантику слов в контексте.

Пример:

```
Вход: "Добрый день"

Выход: Векторное представление, например, [0.12, -0.45, 0.78, ...], где каждый элемент вектора отражает семантические особенности текста.
```

## 7. DIETClassifier (Классификатор на основе DIET)

Что происходит: На этом этапе модель DIET (Dual Intent and Entity Transformer) обучается на извлеченных признаках. DIET классифицирует намерения и извлекает сущности из текста.

Пример:

```
Вход: Векторные представления текста.

Выход: Предсказанное намерение (например, greet) и извлеченные сущности (если есть).
```

## 8. EntitySynonymMapper (Маппер синонимов сущностей)

Что происходит: Если в данных есть сущности, этот компонент сопоставляет синонимы сущностей с их каноническими значениями. Например, "Москва" и "столица России" могут быть сопоставлены с одной сущностью.

Пример:

```
Вход: "Я живу в Москве"

Выход: Сущность город: Москва.
```

## 9. ResponseSelector (Выбор ответов)

Что происходит: Этот компонент обучается выбирать подходящие ответы на основе предсказанных намерений. Он используется для обработки часто задаваемых вопросов (FAQ) или других сценариев, где нужно выбирать ответы из предопределенного набора.

Пример:

```
Вход: Намерение greet.

Выход: Ответ "Привет! Чем могу помочь?".
```

## 10. Сохранение обученной модели

Что происходит: После завершения обучения все компоненты (токенизатор, классификатор, маппер сущностей и т.д.) сохраняются в виде единой модели. Эта модель может быть использована для предсказания намерений и извлечения сущностей в реальном времени.

Пример:

```
Выход: Файл модели, который можно загрузить в Rasa для использования в чат-боте.
```

## Итог

Токенизация: Текст разбивается на слова.


Извлечение признаков: Текст преобразуется в числовые признаки с помощью регулярных выражений, лексико-синтаксического анализа, CountVectors и BERT.

Классификация: Модель DIET обучается на извлеченных признаках для предсказания намерений и извлечения сущностей.

Маппинг сущностей: Синонимы сущностей сопоставляются с каноническими значениями.

Выбор ответов: Модель обучается выбирать подходящие ответы на основе намерений.

Сохранение модели: Все компоненты сохраняются в виде единой модели для дальнейшего использования.
